{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3_13061914_Yiqin_Zhang.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/13061914/UTS_ML2019_Main/blob/master/A3_13061914_Yiqin_Zhang.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UNlqkiJuzBc",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment 3: Take Home Exam - 31005 Machine Learning Spring 2019**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFB_WBabuy-0",
        "colab_type": "text"
      },
      "source": [
        "**Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1Ppxujbuy8q",
        "colab_type": "text"
      },
      "source": [
        "The ensemble method also known as the strong learner, is a solution that can obtain more accurate prediction results in data analysis and produce the best results through the establishment and combination of multiple basic models (week learners). Dasarathy who first proposed the idea of integrated system, used linear classifier and nearest neighbor classifier to compose a composite system as an example to explain and illustrate this idea (Dasarathy, B. V. & Sheela, B. V, 1979). This report will discuss three aspects of the ensemble methods and the pros and cons of their solution, the creation of different classifiers, the decision to merge classifiers, and the weight of each classifier on the ensemble of answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MHPgwQ2uy5L",
        "colab_type": "text"
      },
      "source": [
        "**Create classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjW_WqESuy2T",
        "colab_type": "text"
      },
      "source": [
        "Creating a classifier model is the first step in the integration approach, and choosing the right classifier is the key to this step. The selection of classifier needs to determine the classifier to be used according to the number of samples, number of features, noise in the data set and data linearity. For small training sets, a classifier with high deviation or low variance is more accurate and not easy to fit the data results. As the data set grows, a classifier with low deviation or high variance makes its results more useful. Classifiers could be divided into two types, respectively for two different types of data, the classification model of discrete data and the regression model of continuous data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9chgrwVOuyxr",
        "colab_type": "text"
      },
      "source": [
        "Commonly used classification models are K-Nearest Neighbor (KNN) and Support Vector Machines (SVM). KNN is a model without parameter training, which requires training parameter K to be set and the complexity of the algorithm is high. SVM is an extremely computationally intensive classifier method, which maps the disordered and disorderly data in dimensionality reduction to high-dimensional space through kernel functions and then separates them in hyperplane lines. Therefore, it saves some memory space without calculating all the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUyzs9TKuypj",
        "colab_type": "text"
      },
      "source": [
        "Linear regression and logistic regression are two commonly used regression models. Linear regression assumes a linear distribution of data, which limits the accuracy of the model. Logistic regression is a derived classifier, which also lacks accuracy, but has a slightly shorter computation time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2pKFqjPuyP4",
        "colab_type": "text"
      },
      "source": [
        "In addition, random forest and decision tree can process two different data classifiers. The training speed of random forest is fast, suitable for processing big data and strong anti-noise ability, but not suitable for the characteristics of more data division. The training time of decision tree is long, but the data requirement is low and easy to understand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPYyXdf8vFGA",
        "colab_type": "text"
      },
      "source": [
        "**Merge classifier decision**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67qeBBUcvE4Q",
        "colab_type": "text"
      },
      "source": [
        "Efron,B (1979) proposed bootstrap method, due to its versatility and simplicity, this method has gained strong practicability. As the ensemble method is generated by the combination of several basic models, it can be divided into three categories. Boosting reduces deviation, bagging helps reduce variance, and stacking helps predict the result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Pch4sGvErq",
        "colab_type": "text"
      },
      "source": [
        "Boost method adopts the repeated sampling principle of bootstrapping method, and there is correlation between the generated basic modeling, which represents the algorithms including adaboosting algorithm and Gradient Boosted Decision Tree (GBDT) algorithm. Adaptative boosting is full name of adaboosting, the principle is that if the former classifier is changed into a new classifier, the weight of the correctly classified samples will decrease, while the wrong ones will increase. Adaboost is a weak and strong combinatorial classifier with high accuracy and simple algorithm process, but it only provides a framework. GBDT is a promotion model for solving regression problems, which uses regression tree as the basic classifier and gradient promotion as the learning algorithm to generate decision tree iteratively. GBDT is not easily affected by noise, but it is difficult to carry out training data because of the dependence between weak models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqLCb4D1vEjU",
        "colab_type": "text"
      },
      "source": [
        "The whole process of Bagging is bootstrap aggregating, random forest is an upgraded version of bagging. It not only uses random sampling of bagging, also the repeated sampling principle of bootstrap method, and there is no correlation between basic modeling. In the process of decision making, stochastic forest can process the lost data more strongly and produce data with lower variance, but at the cost of increasing deviation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK2W6OL3vEUU",
        "colab_type": "text"
      },
      "source": [
        "Stacking is a method used to minimize the generalization error of one or more generators, it less used than bagging and boosting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbmV_ESevOX-",
        "colab_type": "text"
      },
      "source": [
        "**Classifier weight**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUvzZzu2vRPI",
        "colab_type": "text"
      },
      "source": [
        "In the ensemble methods, the sequential generation of related basic classifiers can be called serial integration method, and the voting method is used to improve the overall performance. If there is no correlation between the generated basic classifiers, it is called parallel integration method, which reduces errors by average."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc4daH8gvTk6",
        "colab_type": "text"
      },
      "source": [
        "There are two kinds of voting methods, majority voting and weighted voting. Majority voting method is the principle that the category with the largest number of results predicted by multiple basic classifiers for the same sample is the final category, that is the minority is subordinate to the majority. Weighted voting method is to multiply the classified votes of each base classifier by a weight, and then sum the weighted votes of each category. The category corresponding to the maximum value is the final category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGPehA0KvVlE",
        "colab_type": "text"
      },
      "source": [
        "There are also two average methods, simple average method and weighted average method, for regression prediction problems. The simple average method obtains the result by calculating the average prediction of the output results of each basic classifier. The weighted average method works in the same way as the weighted voting method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sod8pfgavXks",
        "colab_type": "text"
      },
      "source": [
        "Both methods average or vote on the results of weak learners, which is easy to understand, but may have a large learning error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZX1mpLvvY9i",
        "colab_type": "text"
      },
      "source": [
        "**Summary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQBkHBXOvauM",
        "colab_type": "text"
      },
      "source": [
        "According to ensemble methods requires training multiple models, the calculation time is long, and the cost is high, while the accuracy is higher than that of a single training model. In the future, it is necessary to develop an algorithm to make the computing time shorter, improve the computing power, and popularize the application of the integrated method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l6wtZP9vcfk",
        "colab_type": "text"
      },
      "source": [
        "**Reference**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7ht7-KiveoC",
        "colab_type": "text"
      },
      "source": [
        "Dasarathy, B. V. & Sheela, B. V. 1979, ‘A composite classifier system design: Concepts and methodology’, *Proceedings of the IEEE*, vol. 67, no. 5, pp. 708-713."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmPy_jO4vwH7",
        "colab_type": "text"
      },
      "source": [
        "Efron, B. 1979, ‘Bootstrap Methods: Another Look at the Jackknife’, *The Annals of Statistics*, vol. 7, no.1, pp.1-26."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzOVUInSvz8i",
        "colab_type": "text"
      },
      "source": [
        "Joseph, R. 2019, *Ensemble methods: bagging, boosting and stacking - Understanding the key concepts of ensemble learning*, Towards Data Science, viewed 7 October 2019, < https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205 >."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oJcHl4Av5KX",
        "colab_type": "text"
      },
      "source": [
        "Mandy, S. 2017, *Types of classification algorithms in Machine Learning*, Medium, viewed 6 October 2019, < https://medium.com/@Mandysidana/machine-learning-types-of-classification-9497bd4f2e14 >."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaf53UFewHT2",
        "colab_type": "text"
      },
      "source": [
        "Necati, D. 2015, *Ensemble Methods: Elegant Techniques to Produce Improved Machine Learning Results*, Toptal Developers, viewed 7 October 2019, < https://www.toptal.com/machine-learning/ensemble-methods-machine-learning >."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r-8CssKwReW",
        "colab_type": "text"
      },
      "source": [
        "Vadim, S. 2017, *Ensemble Learning to Improve Machine Learning Results - How ensemble methods work: bagging, boosting and stacking*, Stats & Bots, viewed 6 October 2019, < https://blog.statsbot.co/ensemble-learning-d1dcd548e936 >.\n",
        "\n"
      ]
    }
  ]
}